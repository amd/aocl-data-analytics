..
    Copyright (C) 2023-2024 Advanced Micro Devices, Inc. All rights reserved.

    Redistribution and use in source and binary forms, with or without modification,
    are permitted provided that the following conditions are met:
    1. Redistributions of source code must retain the above copyright notice,
       this list of conditions and the following disclaimer.
    2. Redistributions in binary form must reproduce the above copyright notice,
       this list of conditions and the following disclaimer in the documentation
       and/or other materials provided with the distribution.
    3. Neither the name of the copyright holder nor the names of its contributors
       may be used to endorse or promote products derived from this software without
       specific prior written permission.

    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
    ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
    WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
    IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,
    INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
    BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,
    OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
    WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
    ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
    POSSIBILITY OF SUCH DAMAGE.

.. _decision_forest_intro:

Decision Forests
****************

Decision forests (also referred to as random forests) are ensembles of decision trees. Decision trees split the feature
domain into rectangular regions and make predictions based on the label values in each region.

A node in the decision tree is terminal if it does not have any child nodes. Let :math:`m` be the index of a terminal
node in the tree.  If we are solving a classification problem with two classes, the proportion of observations in the
second class for node `m` is defined as,

.. math::

   \hat{p}^0_m = \frac{1}{N_m} \sum_{x_i \in R_m} I(y_i = 1)

where :math:`R_m` is the rectangular region corresponding to node :math:`m`, :math:`x_i` is a (multi-dimensional)
observation, :math:`y_i` is a label, and :math:`N_m` is the number of observations in :math:`R_m`.

The node impurity is defined as,

.. math::

   C^{0}_m = Q(\hat{p}^0_m)

where :math:`Q(\hat{p}^0_m)` is a function that quantifies the error in the classification fit, with the property that
if all observations in :math:`R_m` are in the same class, then :math:`Q(\hat{p}^0_m)=0` and we say that the node is
pure.

AOCL-DA supports the following choices of :math:`Q(p)`,

.. math::

   \mathrm{Misclassification\ error: }   & \ 1 - \max(p, 1-p) \\
   \mathrm{Gini\ index: }                & \ 1 - 2 p (1-p)    \\
   \operatorname{Cross-entropy or deviance: } & \ -p \log(p) - (1-p) \log(1-p)

Fitting
--------

Decision tree fitting is performed by growing a tree recursively, starting with a single base node.
Each node has an associated depth, which corresponds to the number of splits required to get to that node from the base node. If the
depth of the node is less than the maximum depth and the node is not pure, we minimize the sum of the node impurities
over each possible split,

.. math::

   C_m(j^*, s^*) = \min_{j, s} \bigg( Q(\hat{p}^1_m(j,s)) + Q(\hat{p}^2_m(j,s)) \bigg)

where

.. math::

   \hat{p}^1_m(j,s) &= \sum_{ \{i \ : x_i(j) \leq s \} } I(y_i = 1) \\
   \hat{p}^2_m(j,s) &= \sum_{ \{i \ : x_i(j) > s \} } I(y_i = 1)

and :math:`j` is the feature index, so that :math:`x_i(j)` is the :math:`j`-th feature of the :math:`i`-th observation.

Then node :math:`m` stops being a terminal node, and two new terminal nodes are created as children of node :math:`m`.
The domains for the new child nodes are defined by splitting :math:`R_m` into two rectangles using the variable index
:math:`j^*` and the threshold :math:`s^*`.

This recursive splitting continues until all terminal nodes are either pure, or have reached the maximum depth.  The
maximum depth is a user-defined parameter and, for decision trees, needs to be selected in such a way that avoids
overfitting.

Decision forest fitting is performed by creating an ensemble of decision trees. The training data for each decision tree in
the ensemble is generated by bootstrapping observations from the training data. The fitting for each decision tree in
the ensemble follows a modified version of the decision tree fitting algorithm. In contrast to standard decision tree
fitting, the candidate set of features for each split is a randomly sampled subset. The subset selection is repeated at
each split so a different subset of candidate features is used on each split. In both decision trees and decision
forests, the order in which features are selected is randomized.  This means that if two different splits have the same
value of :math:`C_m(j, s)`, then whichever value of :math:`j` is sampled first will be the split variable.

Prediction
------------

Decision tree prediction is performed by using the set of split feature indices and thresholds to determine which terminal
node a new observation belongs to.  Once the terminal node has been identified, the prediction is determined by the
proportion of observations with label :math:`1` in the training set: if an observation :math:`x` is associated with a
terminal node :math:`m`, and :math:`\hat{p}^0_m > 0.5`, the predicted label is :math:`1`, otherwise it is :math:`0`.

Decision forest prediction is performed by producing an ensemble of decision tree predictions and using a majority vote to
determine the prediction of the decision forest.

Typical workflow for decision trees and decision forests
--------------------------------------------------------

The following workflow can be used to fit a decision tree or a decision forest model and use it to make predictions,

.. tab-set::

   .. tab-item:: Python
      :sync: Python

      **Decision trees**

      1. Initialize a :func:`aoclda.decision_tree.decision_tree()` object with options set in the class constructor.
      2. Fit the model using :func:`aoclda.decision_tree.decision_tree.fit`.
      3. Evaluate prediction accuracy on test data using :func:`aoclda.decision_tree.decision_tree.score`.
      4. Make predictions using the fitted model using :func:`aoclda.decision_tree.decision_tree.predict`.

      **Decision forests**

      1. Initialize a :func:`aoclda.decision_forest.decision_forest()` object with options set in the class constructor.
      2. Fit the model using the :func:`aoclda.decision_forest.decision_forest.fit`.
      3. Evaluate prediction accuracy on test data using :func:`aoclda.decision_forest.decision_forest.score`.
      4. Make predictions using the fitted model using :func:`aoclda.decision_forest.decision_forest.predict`.

   .. tab-item:: C
      :sync: C

      **Decision trees**

      1. Initialize a :cpp:type:`da_handle` with :cpp:type:`da_handle_type` ``da_handle_decision_tree``.
      2. Pass data to the handle using :ref:`da_tree_set_training_data_? <da_tree_set_training_data>`.
      3. Set optional parameters, such as maximum depth, using :ref:`da_options_set_? <da_options_set>`  (see
         :ref:`options section <opts_decisionforests>`).
      4. Fit the model using :ref:`da_tree_fit_? <da_tree_fit>`.
      5. Evaluate prediction accuracy on test data using :ref:`da_tree_score_? <da_tree_score>`.
      6. Make predictions using the fitted model using :ref:`da_tree_predict_? <da_tree_predict>`.

      **Decision forests**

      1. Initialize a :cpp:type:`da_handle` with :cpp:type:`da_handle_type` ``da_handle_decision_forest``.
      2. Pass data to the handle using :ref:`da_forest_set_training_data_? <da_forest_set_training_data>`.
      3. Set optional parameters, such as maximum depth, using :ref:`da_options_set_? <da_options_set>`  (see
         :ref:`options section <opts_decisionforests>`).
      4. Fit the model using :ref:`da_forest_fit_? <da_forest_fit>`.
      5. Evaluate prediction accuracy on test data using :ref:`da_forest_score_? <da_forest_score>`.
      6. Make predictions using the fitted model using :ref:`da_forest_predict_? <da_forest_predict>`.

Options
-------

.. tab-set::

   .. tab-item:: Python
      :sync: Python

      The available Python options are detailed in the :func:`aoclda.decision_tree.decision_tree` and
      :func:`aoclda.decision_forest.decision_forest` class constructors.

   .. tab-item:: C
      :sync: C

      The following options can be set using :ref:`da_options_set_? <da_options_set>`:

      .. update options using table _opts_decisionforests

      .. csv-table:: :strong:`Table of options for decision forests.`
         :escape: ~
         :header: "Option name", "Type", "Default", "Description", "Constraints"

         "block size", "integer", ":math:`i=256`", "Set the size of the blocks for parallel computations.", ":math:`1 \le i \le 9223372036854775807`"
         "node minimum samples", "integer", ":math:`i=2`", "Minimum number of samples to consider a node for splitting.", ":math:`2 \le i`"
         "scoring function", "string", ":math:`s=` `gini`", "Select scoring function to use.", ":math:`s=` `cross-entropy`, `entropy`, `gini`, `misclass`, `misclassification`, or `misclassification-error`."
         "maximum depth", "integer", ":math:`i=29`", "Set the maximum depth of trees.", ":math:`0 \le i \le 61`"
         "seed", "integer", ":math:`i=-1`", "Set random seed for the random number generator. If the value is -1, a random seed is automatically generated. In this case the resulting classification will create non-reproducible results.", ":math:`-1 \le i`"
         "tree building order", "string", ":math:`s=` `depth first`", "Select in which order to explore the nodes.", ":math:`s=` `breadth first`, or `depth first`."
         "feature threshold", "real", ":math:`r=1e-06`", "Minimum difference in feature value required for splitting.", ":math:`0 \le r`"
         "bootstrap", "string", ":math:`s=` `yes`", "Select whether to bootstrap the samples in the trees.", ":math:`s=` `no`, or `yes`."
         "bootstrap samples factor", "real", ":math:`r=0.8`", "Proportion of samples to draw from the data set to build each tree if 'bootstrap' was set to 'yes'.", ":math:`0 < r \le 1`"
         "features selection", "string", ":math:`s=` `sqrt`", "Select how many features to use for each split.", ":math:`s=` `all`, `custom`, `log2`, or `sqrt`."
         "number of trees", "integer", ":math:`i=100`", "Set the number of trees to compute. ", ":math:`1 \le i`"
         "minimum split score", "real", ":math:`r=0.03`", "Minimum score needed for a node to be considered for splitting.", ":math:`0 \le r \le 1`"
         "maximum features", "integer", ":math:`i=0`", "Set the number of features to consider when splitting a node. 0 means take all the features.", ":math:`0 \le i`"
         "minimum split improvement", "real", ":math:`r=0.03`", "Minimum score improvement needed to consider a split from the parent node.", ":math:`0 \le r`"
         "check data", "string", ":math:`s=` `no`", "Check input data for NaNs prior to performing computation.", ":math:`s=` `no`, or `yes`."
         "storage order", "string", ":math:`s=` `column-major`", "Whether data is supplied and returned in row- or column-major order.", ":math:`s=` `c`, `column-major`, `f`, `fortran`, or `row-major`."





Choosing the decision forest options
""""""""""""""""""""""""""""""""""""

By default, the number of features to use in each split is set to be the square root of the total number of
features.  This can be changed through the ``features selection`` option.  If ``features
selection`` is set to ``custom``, then the value of ``maximum features`` will be used.  Otherwise, the value of
the ``maximum features`` option is ignored.

By default, bootstrap sampling is used, with the number of bootstrap samples set through the ``bootstrap samples
factor`` option.  However, if the value of the ``bootstrap`` option is set to ``no`` then no bootstrapping is
done, i.e., each tree uses the full dataset.

The optimal values of the optional parameters is typically problem dependent.  Cross-validation is typically used to tune options /
hyperparameters.

The memory requirements for decision forests are proportional to :math:`\textit{number of trees} \times \textit{number of bootstrap samples}`.
Reducing the number of bootstrap samples through the ``bootstrap samples factor`` option may
improve performance.  If the dataset is large, reducing ``bootstrap samples factor`` may be needed to ensure
that that all the data required to fit the decision forest can be stored in DRAM.


Examples
--------

.. tab-set::

   .. tab-item:: Python
      :sync: Python

      The code below is supplied with your installation (see :ref:`Python examples <python_examples>`).

      .. collapse:: Decision tree Example

          .. literalinclude:: ../../python_interface/python_package/aoclda/examples/decision_tree_ex.py
              :language: Python
              :linenos:

   .. tab-item:: C
      :sync: C

      The code below can be found in ``decision_tree.cpp`` in the ``examples`` folder of your installation.

      .. collapse:: Decision Forest Example Code

         .. literalinclude:: ../../tests/examples/decision_tree.cpp
            :language: C++
            :linenos:

Further reading
----------------

An introduction to decision trees and to decision forests can be found in Chapters 9 and 15 of :cite:t:`hastie`.

.. toctree::
    :maxdepth: 1
    :hidden:

    decision_tree_api
